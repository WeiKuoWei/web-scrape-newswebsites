{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "import json\n",
    "\n",
    "site_list = [\"bbc\", \"cnn\", \"foxnews\", \"nationalreview\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file\n",
    "def open_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # return data \n",
    "\n",
    "    # file_path = \"data/\" + site_list[0] + \"/articles.json\"\n",
    "\n",
    "    json_data = open_json(file_path)\n",
    "    data = [{\"url\": url, **details} for url, details in json_data.items() if details is not None]\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df_selected = df[[\"title\", \"date_publish\", \"description\", \"maintext\", \"authors\", \"wayback_id\"]]\n",
    "    # df_selected.describe()\n",
    "    return df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDF(df_selected):\n",
    "    # clean the data\n",
    "    # drop the main text that is empty\n",
    "    df_selected = df_selected.dropna(subset=[\"maintext\"])\n",
    "\n",
    "    # drop the duplicates\n",
    "    df_selected = df_selected.drop_duplicates(subset=[\"maintext\"])\n",
    "\n",
    "    # add a new column based on the wayback_id\n",
    "    df_selected[\"wayback_time\"] = pd.to_datetime((df_selected[\"wayback_id\"]).astype(str).str[:8], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # add a new column based on the length of the main text\n",
    "    df_selected[\"text_len\"] = df_selected[\"maintext\"].apply(lambda x: len(x))\n",
    "\n",
    "    df_selected.drop(columns=[\"wayback_id\"], inplace=True)\n",
    "\n",
    "    # sort\n",
    "    df_selected = df_selected.sort_values(by=\"wayback_time\", ascending=True)\n",
    "\n",
    "    # print(df_selected.describe())\n",
    "    # print(df_selected.head())\n",
    "    # df_selected.head()\n",
    "    return df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data back on title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphDF(df_selected):\n",
    "    df_selected['year_week'] = df_selected['wayback_time'].dt.strftime('%Y-%U')\n",
    "    df_selected['week_start_date'] = pd.to_datetime(df_selected['year_week'] + '-0', format='%Y-%U-%w')\n",
    "\n",
    "    # sort the dataframe by date\n",
    "    df_selected_weekly = df_selected.groupby('week_start_date')['title'].count().reset_index()\n",
    "    df_selected_year_week = df_selected.groupby('year_week').count().reset_index()\n",
    "    df_selected_year_week.sort_values('year_week', inplace=True)\n",
    "\n",
    "    # plot the number of urls per day\n",
    "    fig = px.line(\n",
    "        df_selected_weekly, \n",
    "        x='week_start_date', \n",
    "        y='title',\n",
    "        labels={'week_start_date': 'Week Starting', 'title': 'Number of Articles'},\n",
    "        template='seaborn'\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Week Starting',\n",
    "        yaxis_title='Number of Articles',\n",
    "        title='Number of Articles per Week',\n",
    "        title_x=0.5\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x11819bfb0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/weikuo/Documents/github-repositories/web-scrape-newswebsites/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "for site in site_list[:1]:\n",
    "    file_path = \"data/\" + site + \"/articles.json\"\n",
    "    df_selected = open_json(file_path)\n",
    "    df_selected = cleanDF(df_selected)\n",
    "    graphDF(df_selected)\n",
    "    print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
